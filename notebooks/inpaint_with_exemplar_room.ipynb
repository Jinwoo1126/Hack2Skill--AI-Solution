{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import kornia\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd()),\n",
    "sys.path.append('./unipaint/')\n",
    "sys.path.append('./unipaint/src/clip')\n",
    "sys.path.append('./unipaint/src/taming-transformers')\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.util import instantiate_from_config, load_model_from_config, ExemplarAugmentor\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompt_values import ImageURL\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from google.cloud import vision\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100  # num of fine-tuning iterations\n",
    "lr = 1e-5\n",
    "config_path = \"unipaint/configs/stable-diffusion/v1-inference.yaml\"\n",
    "ckpt_path = \"unipaint/ckpt/sd-v1-4-full-ema.ckpt\"  # path to SD checkpoint\n",
    "h = w = 512\n",
    "scale=24  # cfg scale\n",
    "ddim_steps= 50\n",
    "ddim_eta=0.0\n",
    "# seed_everything(42)\n",
    "n_samples = 2\n",
    "out_path = \"outputs/\" \n",
    "gpu_id = '0'\n",
    "GOOGLE_GEMINI_API_KEY = \"\"\n",
    "GOOGLE_VISION_API_KEY = \"\"\n",
    "PID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tensor as image file\n",
    "def tsave(tensor, save_path, **kwargs):\n",
    "    save_image(tensor, save_path, normalize=True, scale_each=True, value_range=(-1, 1), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get bounding box and generating mask for sequential inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagsForList(BaseModel):\n",
    "    \n",
    "    furniture_list: list[str] = Field(..., description = \"List of furnitures\");\n",
    "\n",
    "class Localizer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.GOOGLE_GEMINI_API_KEY = GOOGLE_GEMINI_API_KEY\n",
    "        self.GOOGLE_VISION_API_KEY = GOOGLE_VISION_API_KEY\n",
    "        self.PID = PID\n",
    "        self.llm = ChatGoogleGenerativeAI(model = 'gemini-pro', google_api_key = self.GOOGLE_GEMINI_API_KEY,\n",
    "                                         temperature = 0)\n",
    "\n",
    "        self.parser = PydanticOutputParser(pydantic_object = TagsForList)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template = \"\"\"Answer the user query. \\n {format_instructions}\\n{query}\\n\n",
    "            You select only the items from the list that can be categorized as furniture or appliances.\n",
    "            \"\"\",\n",
    "            input_variables = [\"query\"],\n",
    "            partial_variables = {\"format_instructions\" : self.parser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "        self.furnitures = \"\"\n",
    "\n",
    "    def localize_objects(self, url:str, api_key:str = None, pid:str = None):\n",
    "\n",
    "        if api_key is None:\n",
    "            api_key = self.GOOGLE_VISION_API_KEY\n",
    "\n",
    "        if pid is None:\n",
    "            pid = self.PID\n",
    "\n",
    "        client = vision.ImageAnnotatorClient(\\\n",
    "            client_options = {\"api_key\": self.GOOGLE_VISION_API_KEY, \"quota_project_id\": pid})\n",
    "    \n",
    "        res = requests.get(url)\n",
    "        img = vision.Image(content = res.content)\n",
    "        \n",
    "        objects = client.object_localization(image = img).localized_object_annotations\n",
    "    \n",
    "        obj_list = []\n",
    "        upper_left_axis_list = []\n",
    "        bottom_right_axis_list = []\n",
    "        \n",
    "        for object_ in objects:\n",
    "            \n",
    "            obj_list.append(object_.name)\n",
    "    \n",
    "            for i, vertex in enumerate(object_.bounding_poly.normalized_vertices):\n",
    "                \n",
    "                if i == 0:\n",
    "                    upper_left_axis_list.append((vertex.x, vertex.y))\n",
    "    \n",
    "                if i == 2:\n",
    "                    bottom_right_axis_list.append((vertex.x, vertex.y))\n",
    "    \n",
    "        return dict(zip(obj_list, upper_left_axis_list)), dict(zip(obj_list, bottom_right_axis_list))\n",
    "    \n",
    "    def query(self, url:str):\n",
    "\n",
    "        upper_left_axis, bottom_right_axis = self.localize_objects(url = url)\n",
    "\n",
    "        query_sentence = str(list(upper_left_axis.keys()))\n",
    "\n",
    "        chain = self.prompt | self.llm | self.parser\n",
    "        llm_output = chain.invoke({\"query\" : query_sentence})\n",
    "        self.furnitures = llm_output.furniture_list\n",
    "\n",
    "        upper_left_axis = {key: upper_left_axis[key] for key in self.furnitures}\n",
    "        bottom_right_axis = {key: bottom_right_axis[key] for key in self.furnitures}\n",
    "\n",
    "        return upper_left_axis, bottom_right_axis\n",
    "\n",
    "\n",
    "def gen_masks(url:str, bounding_box_axis_top_left:dict, bounding_box_axis_bottom_right:dict, save_fn = None, expanding_factor:float = 1.05):\n",
    "\n",
    "    if save_fn is None:\n",
    "        save_fn = url.split('/')[-1] + '.jpg'\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    img = Image.open(BytesIO(res.content))\n",
    "    img.save(save_fn + '.jpg')\n",
    "\n",
    "    rect_axises = []\n",
    "    dict_json = {}\n",
    "    \n",
    "    for obj_nm, top_left_tup, bottom_right_tup in zip(bounding_box_axis_top_left.keys(), bounding_box_axis_top_left.values(), bounding_box_axis_bottom_right.values()):\n",
    "\n",
    "        # draw a black box\n",
    "        width = img.width; height = img.height\n",
    "        img_mask = Image.new(\"RGB\", (width, height), \"black\")\n",
    "        draw = ImageDraw.Draw(img_mask)\n",
    "    \n",
    "        rect = (int(top_left_tup[0]*width/expanding_factor),\\\n",
    "                int(top_left_tup[1]*height/expanding_factor),\\\n",
    "                int(bottom_right_tup[0]*width*expanding_factor),\\\n",
    "                int(bottom_right_tup[1]*height*expanding_factor))\n",
    "\n",
    "        if rect not in rect_axises:\n",
    "        \n",
    "            rect_axises.append(rect)\n",
    "            draw.rectangle(rect, fill = \"white\")\n",
    "            img_mask.save(save_fn + f'_{obj_nm}' + '_mask.jpg')\n",
    "            dict_json[obj_nm] = save_fn + f'_{obj_nm}' + '_mask.jpg'\n",
    "\n",
    "    with open(save_fn + \".json\", \"w\") as json_file:\n",
    "        json.dump(dict_json, json_file)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://img.maisonkorea.com/2020/03/msk_5e65a1179ab47.jpg',\n",
    "        'https://cdn.ggumim.co.kr/cache/star/600/20201222151726uFqauJF8wD.jpg',\n",
    "        'https://img.maisonkorea.com/2020/03/msk_5e659f7c6816a.jpg']\n",
    "\n",
    "GOOGLE_GEMINI_API_KEY = \"\"\n",
    "GOOGLE_VISION_API_KEY = \"\"\n",
    "PID = \"104910716278680354156\"\n",
    "\n",
    "lc = Localizer()\n",
    "\n",
    "for j, url in enumerate(urls):\n",
    "    bb_dict_top_left, bb_dict_bottom_right = lc.query(url)\n",
    "    gen_masks(url = url,\\\n",
    "              bounding_box_axis_top_left = bb_dict_top_left,\\\n",
    "              bounding_box_axis_bottom_right = bb_dict_bottom_right,\\\n",
    "              save_fn = f'./temp/sample_{j:02d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplar inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments'\n",
    "\n",
    "def inpaint(config_path, ckpt_path, item_type, org_image_path:str, mask_path:str,\n",
    "            ref_path:str, out_path:str, item_desc:str = \"\", save_fn:str = \"\", tuning = False):\n",
    " \n",
    "    # set config\n",
    "    \n",
    "    config = OmegaConf.load(config_path)\n",
    "    config.model.params.personalization_config.params.initializer_words = [item_type] \n",
    "    config.model.params.personalization_config.params.initializer_images = [ref_path] \n",
    "\n",
    "    device = torch.device(f\"cuda:{gpu_id}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # load model\n",
    "    \n",
    "    model = load_model_from_config(config, ckpt_path, device)\n",
    "    sampler = DDIMSampler(model)\n",
    "    params_to_be_optimized = list(model.model.parameters())\n",
    "    optimizer = torch.optim.Adam(params_to_be_optimized, lr=lr)\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # load imgs from path\n",
    "    \n",
    "    image = Image.open(org_image_path).convert('RGB').resize((h,w), Image.Resampling.BILINEAR)\n",
    "    mask = Image.open(mask_path).convert('L').resize((h,w), Image.Resampling.BILINEAR)\n",
    "    image_ref = Image.open(ref_path).convert('RGB').resize((h,w), Image.Resampling.BILINEAR)\n",
    "\n",
    "    # define enc/dec\n",
    "\n",
    "    D = lambda _x: torch.clamp(model.decode_first_stage(_x), min=-1, max=1).detach() # vae decode\n",
    "    E = lambda _x: model.get_first_stage_encoding(model.encode_first_stage(_x))  # # vae encode\n",
    "    img_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.unsqueeze(0) * 2. - 1)])\n",
    "    mask_transforms = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x.unsqueeze(0) > 0).float())])\n",
    "    \n",
    "    # define text encoder\n",
    "    def C(_txt, enable_emb_manager=False):\n",
    "        _txt = [_txt] if isinstance(_txt,str) else _txt\n",
    "        with torch.enable_grad() if enable_emb_manager else torch.no_grad(): # # disable grad flow unless we want textual inv\n",
    "            c = model.get_learned_conditioning(_txt, enable_emb_manager)\n",
    "            return c\n",
    "\n",
    "    # transform imgs\n",
    "    \n",
    "    x = img_transforms(image).to(device)\n",
    "    m = mask_transforms(mask).to(device)\n",
    "    x_ref = img_transforms(image_ref).to(device)\n",
    "\n",
    "    x_in = x * (1 - m)\n",
    "    z_xm = E(x_in)\n",
    "    z_ref = E(x_ref)\n",
    "    z_m = F.interpolate(m, size=(h // 8, w // 8))\n",
    "    z_m = kornia.morphology.dilation(z_m, torch.ones((3,3),device=device))\n",
    "\n",
    "    attn_mask = {}\n",
    "    for attn_size in [64,32,16,8]:  # create attention masks for multi-scale layers in unet\n",
    "        attn_mask[str(attn_size**2)]= (F.interpolate(m, (attn_size,attn_size), mode='bilinear'))[0,0,...]\n",
    "\n",
    "    # fine tuning\n",
    "    if tuning:\n",
    "\n",
    "        exemplar_augmentor = ExemplarAugmentor(mask=mask)\n",
    "        \n",
    "        model.train()\n",
    "        pbar = tqdm(range(70), desc='Fine-tune the model')\n",
    "        for i in pbar:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            x_reff, x_reff_mask = exemplar_augmentor(x_ref)\n",
    "            z_reff = E(x_reff)\n",
    "            z_reff_mask = F.interpolate(x_reff_mask, size=(h // 8, w // 8),mode='bilinear')\n",
    "        \n",
    "            t_emb = torch.randint(model.num_timesteps, (1,), device=device)\n",
    "            c_ref =  C(item_desc,True).detach()\n",
    "            uc = C(\"\").detach()\n",
    "            noise1 = torch.randn_like(z_xm)\n",
    "            z_ref_t = model.q_sample(z_reff, t_emb, noise=noise1)\n",
    "            pred_noise_ref = model.apply_model(z_ref_t, t_emb, c_ref)\n",
    "            loss_ref = F.mse_loss(pred_noise_ref * z_reff_mask, noise1 * z_reff_mask)\n",
    "        \n",
    "            t_emb2 = torch.randint(model.num_timesteps, (1,), device=device)\n",
    "            noise2 = torch.randn_like(z_xm)\n",
    "            z_bg_t = model.q_sample(z_xm, t_emb2, noise=noise2)\n",
    "            pred_noise_bg = model.apply_model(z_bg_t, t_emb2, uc)\n",
    "            loss_bg = F.mse_loss(pred_noise_bg * (1 - z_m), noise2 * (1 - z_m))\n",
    "        \n",
    "            loss = loss_bg + loss_ref\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses_dict = {\"loss\": loss,  \"loss_bg\": loss_bg, \"loss_ref\":loss_ref}\n",
    "            pbar.set_postfix({k: v.item() for k,v in losses_dict.items()})\n",
    "        \n",
    "    # gen imgs\n",
    "    \n",
    "    with torch.no_grad(), torch.autocast(device.type):\n",
    "        tmp, _ = sampler.sample(S=ddim_steps, batch_size=n_samples, shape=[4, h // 8, w // 8],\n",
    "                            conditioning=C(item_desc, True).repeat(n_samples,1,1), \n",
    "                            blend_interval=[0, 1], \n",
    "                            x0=z_xm.repeat(n_samples,1,1,1), \n",
    "                            mask=z_m.repeat(n_samples,1,1,1), \n",
    "                            attn_mask=attn_mask,\n",
    "                            x_T=None, \n",
    "                            unconditional_guidance_scale=scale, \n",
    "                            eta=ddim_eta,\n",
    "                            verbose=False)\n",
    "\n",
    "    tsave(D(tmp), os.path.join(out_path, save_fn), nrow=n_samples)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "initializer_words is desk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune the model: 100%|█████████| 150/150 [2:22:24<00:00, 56.96s/it, loss=0.0104, loss_bg=0.00782, loss_ref=0.00263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (2, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|████████████████████████████████████████████████████████████████████| 50/50 [04:50<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blend_interval = [0, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = inpaint(item_type = 'desk',\n",
    "        config_path = config_path,\n",
    "        ckpt_path = ckpt_path,\n",
    "        org_image_path = './temp/sample_00.jpg',\n",
    "        mask_path = './temp/sample_00_Desk_mask.jpg',\n",
    "        ref_path = './data/desk/thumbnails_nobg/50.jpg',\n",
    "        item_desc = 'modern black desk in cozy room',\n",
    "        out_path = './outputs/',\n",
    "       save_fn = 'sample_00_st_01_D.jpg',\n",
    "       tuning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "initializer_words is desk\n",
      "Data shape for DDIM sampling is (2, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|████████████████████████████████████████████████████████████████████| 50/50 [03:16<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blend_interval = [0, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = inpaint(item_type = 'desk',\n",
    "        config_path = config_path,\n",
    "        ckpt_path = ckpt_path,\n",
    "        org_image_path = './temp/sample_00.jpg',\n",
    "        mask_path = './temp/sample_00_Desk_mask.jpg',\n",
    "        ref_path = './data/desk/thumbnails_nobg/50.jpg',\n",
    "        item_desc = '#',\n",
    "        out_path = './outputs/',\n",
    "       save_fn = 'sample_00_st_01_E.jpg',\n",
    "       tuning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
